!pip install langchain transformers sentence-transformers faiss-cpu
!pip install -U langchain langchain-community


import os

# ì´ê±´ êµ¬ê¸€ colab ì— ì—°ë™í• ë•Œë§Œ í•„ìš”í•œ ì½”ë“œì—ìš”
# â–¶ Colab ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸ ë° ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
from google.colab import drive
drive.mount('/content/drive')
# ì´ê±´ êµ¬ê¸€ colab ì— ì—°ë™í• ë•Œë§Œ í•„ìš”í•œ ì½”ë“œì—ìš” END


# ë“œë¼ì´ë¸Œ ìƒì˜ í´ë” ê²½ë¡œ ì„¤ì •
folder_path = "/content/drive/MyDrive/dataset/text"

# í´ë” ë‚´ ëª¨ë“  .txt íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ ê°€ì ¸ì˜¤ê¸°
file_list = [os.path.join(folder_path, fname) for fname in os.listdir(folder_path) if fname.endswith(".txt")]

# ëª¨ë“  íŒŒì¼ ë‚´ìš© ì½ì–´ì„œ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
texts = []
for file_path in file_list:
    with open(file_path, 'r', encoding='utf-8') as f:
        texts.append(f.read())
print(texts)


# â–¶ ë¬¸ì„œ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸°
from langchain.text_splitter import CharacterTextSplitter
splitter = CharacterTextSplitter(
    separator="\n",
    chunk_size=500,
    chunk_overlap=50,
    length_function=len
)

# ì—¬ëŸ¬ ê°œì˜ ë¬¸ì„œë¥¼ í•œë²ˆì— ì²­í¬ë¡œ ë‚˜ëˆ„ê¸°
documents = splitter.create_documents(texts)


# â–¶ ì„ë² ë”© ë° FAISS êµ¬ì¶•
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)
vector_db = FAISS.from_documents(documents, embedding_model)
save_path = "/content/drive/MyDrive/embedding/rag"
vector_db.save_local(save_path)




# ì‚¬ìš© ì˜ˆì œ
# â–¶ FAISS ì¬ë¡œë“œ ë° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸    save_path = "/content/drive/MyDrive/embedding/rag"
vector_db = FAISS.load_local(save_path, embedding_model, allow_dangerous_deserialization=True)
retriever = vector_db.as_retriever(search_kwargs={"k": 8})

# â–¶ Gemini API ì—°ê²°
import google.generativeai as genai
genai.configure(api_key="AIzaSyCj_KP3tWm9_3cgbjvQrZl5vv2M3_DBfZ0")

model = genai.GenerativeModel("gemini-2.5-flash")

# â–¶ ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ ë° QA ì²´ì¸ ì •ì˜ (LangChain ì—†ì´ ì§ì ‘)
def gemini_rag_answer(query):
    docs = retriever.get_relevant_documents(query)
    context = "\n".join([doc.page_content for doc in docs])

    prompt = f"""
ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” AIì…ë‹ˆë‹¤.
ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µí•˜ì‹­ì‹œì˜¤:

ë¬¸ì„œ:
{context}

ì§ˆë¬¸:
{query}

ë‹µë³€:
"""
    response = model.generate_content(prompt)
    return response.text

# â–¶ í…ŒìŠ¤íŠ¸
query = "ë­˜ íŒŒë‹ˆ"
answer = gemini_rag_answer(query)
print("ğŸ“Œ ì§ˆë¬¸:", query)
print("ğŸ¤– Gemini ë‹µë³€:", answer)
# ì‚¬ìš© ì˜ˆì œ END
